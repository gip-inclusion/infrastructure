#!/usr/bin/env python

import hashlib
import logging
import os
import sys
from pathlib import Path
from typing import List, Tuple

import jinja2

logging.basicConfig(stream=sys.stdout, level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger("__name__")

templateLoader = jinja2.FileSystemLoader(searchpath=Path(__file__).parent)
templateEnv = jinja2.Environment(loader=templateLoader, undefined=jinja2.StrictUndefined)

CURRENT_DIRECTORY = os.getcwd()
INFRASTRUCTURE_DIRECTORY = f"{CURRENT_DIRECTORY}/infrastructure"
CI_DIRECTORY = ".github/workflows"
WORKFLOWS_PREFIX = "infrastructure_"
EVENT_TYPES = ["push", "pull_request"]
IGNORED_DIRS = [".git", ".terraform", "_modules"]


def slugify(value: str) -> str:
    return value.translate(str.maketrans("/-", "__"))


def write_pipeline_file(project_name: str) -> set[str]:
    file_names = set()

    template = templateEnv.get_template(f"terraform.yaml.jinja")

    for event_type in EVENT_TYPES:
        project_slug = slugify(project_name)            
        target = "pr" if event_type == "pull_request" else "main"
        workflow_name = f"{'Apply' if target == 'main' else 'Plan'}: {project_name}"
        job_name = f"{project_slug}_terraform_{event_type}"
        file_name = f"{CI_DIRECTORY}/{job_name}.yaml"

        if len(job_name) > 100:
            # Github job names have a maximum length of 100 chars
            # Truncate and append a stable hash to ensure uniqueness
            job_name = f"{job_name[:90].rsplit('_', 1)[0]}_{hashlib.sha1(job_name.encode()).hexdigest()[:8]}"

        workflow = template.render(
            resource_name=f"gip-inclusion-{project_slug}",
            project_path=f"{project_name}/terraform",
            project=project_slug,
            event_type=event_type,
            workflow_name=workflow_name,
            job_name=job_name,
            target=target,
        )
        
        with open(file_name, "w") as f:
            f.write(workflow)
            logger.info(f"Created file `{file_name}` for project `{project_name}`")
            file_names.add(file_name)

    return file_names


def discover_infrastructure(folder_path: str) -> set[str]:
    workflow_files = set()
    base_parents = Path(CURRENT_DIRECTORY).parents

    for dirpath, dirnames, filenames in Path(folder_path).walk("*"):
        # Remove directories that should not be processed
        for dir_name in IGNORED_DIRS:
            if dir_name in dirnames:
                logger.debug(f"Removing ignored directory: {dir_name} in {dirpath}")
                dirnames.remove(dir_name)

        project_dir = dirpath.parent
        # Skip if object depth crosses the threshold
        object_parents = set(base_parents) ^ set(dirpath.parents)
        if len(object_parents) > 10:
            logger.warning(
                f"{dirpath.relative_to(CURRENT_DIRECTORY)} is too deep in the directory tree, skipping."
            )
            continue

        if dirpath.name in ("terraform",):
            logger.debug(f"Found {dirpath.name} folder in {project_dir}")
            relative_project_path = str(Path(project_dir).relative_to(CURRENT_DIRECTORY))
            workflow_files |= write_pipeline_file(relative_project_path)

    return workflow_files


if __name__ == "__main__":
    logger.info(f"Deleting previously generated files in {CI_DIRECTORY}")
    # Delete only the files that are generated by this script
    for file_path in Path(CI_DIRECTORY).glob(f"{WORKFLOWS_PREFIX}*"):
        if file_path.is_file():
            file_path.unlink()

    logger.info(f"Starting to discover the following folder structure: {INFRASTRUCTURE_DIRECTORY}")
    workflow_files = discover_infrastructure(INFRASTRUCTURE_DIRECTORY)
    logger.info(f"{len(workflow_files)} workflows created")
